{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is general term for two units, Storage and Processing. Storage component is HDFS and Processing component is Yarn.\n",
    "\n",
    "HDFS allows you to dump any kind of data across the cluster. Yarn allows parallel processing on the data. It is data intensive,  because code is transferred to the data location.\n",
    "\n",
    "MapReduce: Map for parallel processing and Reduce for aggregation.\n",
    "\n",
    "Pig has two parts, pig latin - language and Pig runtime - Execution engine.\n",
    "\n",
    "Whenever you write pig script there will be MapReduce job running in the background.\n",
    "\n",
    "Hive - Powerful Analytical tool, uses HiveQL.\n",
    "\n",
    "Spark - very popular for real-time data processing. Written in scala. Executes In-memory computations. 100 times faster than MapReduce.\n",
    "\n",
    "Oozie - Schedules hadoop jobs.\n",
    "\n",
    "Flume - Ingestion tools for unstructured data.\n",
    "\n",
    "Sqoop - Ingestion tool for Strutured data.\n",
    "\n",
    "HDFS - Hadoop Distributed File System. HDFS has NameNode and DataNode.\n",
    "\n",
    "NameNode - Master Daemon. Maintains and manages data nodes. Stores meta data like location of blocks, size of the blocks, permissions, hierarchy and receives heart beat from data nodes.\n",
    "\n",
    "*Secondary NameNode:*\n",
    "\n",
    "Checkpointing is a process of capturing editlogs with Fslimage.\n",
    "\n",
    "Allows faster failover as we have backup of metadata.\n",
    "\n",
    "checkpoint happens periodically. Default once per hour.\n",
    "\n",
    "*FslImage:* File that contains state of the file system since the starting of the name node. Records meta data about all the transactions happened on the data eversince name node is set up.\n",
    "\n",
    "*EditLog* Recent modifications you made for the file systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is datawarehouse tool on top of HDFS. It creates tables on top of HDFS. You use hive when you are dealing with structured data and you want to avoid heavy java code.\n",
    "\n",
    "Whatever you load into HDFS they will be stored in the form of flat files. \n",
    "\n",
    "HDFS is schema on Read. RDBMS is schema on write. In RDBMS you need to specify the schema before storing the data. When it comes to HDFS you store the data in the form of Flat files. HDFS doesnt maintain any schema.\n",
    "\n",
    "Hive was created by facebook in 2008, and donated to Apache foundation. \n",
    "\n",
    "Metastore is RDBMS maintained by Hive. Metastore is created on top of HDFS files to give the structure to flat files stored in HDFS. \n",
    "\n",
    "When can create tables on top of HDFS files, you are giving structure to HDFS files by hive, to maintain structure information we use metastore.\n",
    "\n",
    "Whatever the query you are writing in the hive, it is internally converts into MapReduce and runs on Hadoop. \n",
    "\n",
    "Metastore is central repository of the hive metadata. Hive tables, database definitions and mapping to data in HDFS is stored in metastore.\n",
    "\n",
    "Hive metastore can be configured in 3 ways. 1)embedded metastore, 2)local metastore, 3)remote metastore.\n",
    "\n",
    "\n",
    "**Uses of Hive**\n",
    "\n",
    "Hive provides tools to Extract Transform and Load (ETL)\n",
    "\n",
    "Provides structure on variety of data formats.\n",
    "\n",
    "Used for OLAP.\n",
    "\n",
    "By using hive we can access files stored in HBase and HDFS.\n",
    "\n",
    "\n",
    "**Limitations of Hive**\n",
    "\n",
    "It is not a relational database. It is not OLTP. It is not a procedural language. It doesnt have triggers, stored procedures etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tez vs MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig/Hive jobs can be written in SQL and they are converted to MapReduce jobs and run on top of the HDFS.\n",
    "\n",
    "Tez takes place of MapReduce, Instead of translating your queries into mappers and reducers, it translates it into Directed Acyclic Graphs (DAG). It is same idea that spark uses to run faster.\n",
    "\n",
    "Tez is default on EMR. But you can set configuration to MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot stop the clusters in EMR like EC2. \n",
    "\n",
    "In EMR you can select what software you want like Spark, Hive, Hue etc., and EMR configures all the software automatically for you. You just specify how many no. of instances you need.\n",
    "\n",
    "Whereas in EC2 you need to install and configure all your softwares, Name nodes and data nodes etc.,\n",
    "\n",
    "Terminated cluster metadata is stored for 2 days and you can clone it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Machine  - Can scale vertically\n",
    "\n",
    "**Distributed Machine**\n",
    "\n",
    "Horizontal Scaling,\n",
    "\n",
    "Commodity Hardware used in datanodes.\n",
    "\n",
    "Has access to computational resources across the number of machines connected through a network.\n",
    "\n",
    "Fault tolerance; if one of the machine fails, while network can still work.\n",
    "\n",
    "Replicating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a way to distribute very large files across multiple machines.\n",
    "\n",
    "It has two components HDFS and MapReduce, HDFS which duplicates the data for fault tolerance. MapReduce allow computation on distributed data.\n",
    "\n",
    "128Mb default block size. default replication factor is 3.\n",
    "\n",
    "MapReduce: It is a way of splitting computational task to a distributed set of files. It consists of Job Tracker and Task Tracker.\n",
    "\n",
    "Job tracker sends code to run on the task tracker.\n",
    "\n",
    "Task Tracker allocates CPU and memory to the tasks and monitor the tasks on the worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark vs MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to MapReduce.\n",
    "\n",
    "MapReduce writes most of the data to disk after each map and reduce operation, spark keeps data in-memory after each transformatu=ion. Spark can spill over data to disk if the memory is filled.\n",
    "\n",
    "It can use data stored in variety of formats, and databases like Cassandra, S3, HDFS etc., Where as MapReduce can access data stored only in HDFS.\n",
    "\n",
    "only for Batch processing -- Batch processing as well as real time processing.\n",
    "\n",
    "Slower than spark because of its I/O latenct -- 100x faster in-memory and 10x faster on disk.\n",
    "\n",
    "Data processing enginer -- Data analytics enginer\n",
    "\n",
    "Supports SQL through HiveSQL -- Supports SQL through spark SQL\n",
    "\n",
    "MapReduce is not interactive -- Spark is interactive\n",
    "\n",
    "More lines of code -- Less lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-Memory Computing: Keeping data in server's RAM as it makes is easy to access data and makes Machine learning algorithms to work faster.\n",
    "\n",
    "Lazy Evaluation Execution will not start until action is triggered.\n",
    "\n",
    "Supports multiple Languages: Spark allows you to write applications on Java, Python, Scala and R.\n",
    "\n",
    "100x faster\n",
    "\n",
    "Advanced Analytics: Spark not only supports 'Map' and 'Reduce' it also supports SQL queries, Streaming data, Machine Learning Algorithms, and Graph algorithms.\n",
    "\n",
    "Real-Time Processing Spark can handle real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation vs Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two types of operations in spark are Transformation and Action.\n",
    "\n",
    "Transformation is a function that produces new RDD from existing RDD but when we want to work with existing dataset at that point action is performed.\n",
    "\n",
    "• map(), filter(), union()  ---> Transformations\n",
    "\n",
    "• reduce(), collect(), count() ---> Actions.\n",
    "\n",
    "**Spark Dataframes are standard way of using spark Machine Learning Capabilities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is distributed data processing engine that used to process large amounts of data.\n",
    "\n",
    "On top of data processing engine there are libraries for SQL, machine learning, graph computation and stream processing, which can be used together in an application.\n",
    "\n",
    "Programming languages supported by spark include, Java, Python, Scala and R.\n",
    "\n",
    "Tasks most frequently associated with spark include ETL and SQL batch jobs across large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff073c74b5db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findspark library adds pyspark to sys.path. you can specify the location of pyspark with init method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a00465ab9cfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/kalyan/spark-2.4.4-bin-hadoop2.7'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'findspark' is not defined"
     ]
    }
   ],
   "source": [
    "findspark.init('/home/kalyan/spark-2.4.4-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is unified entry point into spark application. Spark session is combination of different contexts like \"Spark Context\", \"hive context\", \"SQL context\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Dataframes\n",
    "\n",
    "spark dataframes store data in row column format. From spark 2.0, spark shifted to dataframes instead of RDDs. \n",
    "\n",
    "Using spark dataframes you can able to input, output and Transform data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appName() sets name for application.\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "               StructField('name', StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json', schema=final_struc)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets column object\n",
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3c0267b1b936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# actually want to get data frame use select method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# you will have greater flexibility to transform data with dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# actually want to get data frame use select method\n",
    "# you will have greater flexibility to transform data with dataframe\n",
    "df.select('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will get list of row objects, you can index it\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing\n",
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method creates a new column in the data.\n",
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above operation is not inplace, \n",
    "# we have to save the above code to a variable\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "df.withColumnRenamed('age', 'my_new_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1294963126dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'people'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT * FROM people WHERE age = 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train your ML models with python in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs in spark are fundamental data structure in spark but dataframes are new data structures. Dataframes are easy to read and performs very better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does pyspark API do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads data into memory.\n",
    "\n",
    "Reads and manipulates data in Spark.\n",
    "\n",
    "Push processing to cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn in spark\n",
    "\n",
    "\n",
    "Opensource machine learning libraries like sklearn do not scale, they dont work in spark. They are written to perform processing only on single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Million Records!\n",
    "\n",
    "You can train your dataset with 10 million records on your local system. You have to use big data tools like spark in order to distribute the processing to different nodes. But opensource Machine learning modules wont work(Scale) with distributed processing.\n",
    "\n",
    "Ypou have to recode everything using Spark machine learning packages, but they are not powerful as sklearn.\n",
    "\n",
    "spark.mllib and spark.ml are both spark's Machine Learning libraries. Spark MLlib is old library primarily made for RDDs. It will be deprecated in new release. spark.ml works well with dataframes\n",
    "\n",
    "The solution is you need to convert your python code into User Defined Functions (UDFs) to apply to each row of spark object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pandas UDFs to train big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/session/automating-predictive-modeling-at-zynga-with-pyspark-and-pandas-udfs\n",
    "\n",
    "https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873\n",
    "\n",
    "https://towardsdatascience.com/scalable-python-code-with-pandas-udfs-a-data-science-application-dd515a628896\n",
    "\n",
    "https://changhsinlee.com/pyspark-udf/\n",
    "\n",
    "https://towardsdatascience.com/beginners-guide-to-create-first-end-to-end-machine-learning-pipeline-in-pyspark-d3df25a08dfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas UDF (User Defined Functions) is a feature that enable python to run in a distributed environment even if the library was developed for the single node execution.\n",
    "\n",
    "Pandas UDF helps to scale up to a large cluster and run your python code in a parallelized and distributed fashion.\n",
    "\n",
    "We first performed task on the driver node and then scale up to the full dataset using pandas UDFs to handle billions of records of data.\n",
    "\n",
    "Pandas UDFs can be used in variety of applications ranging from feature generation to statistical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required libararies both pyspark and python libraries.\n",
    "\n",
    "Load dataset with pandas.\n",
    "\n",
    "Convert the pandas dataframe to the spark dataframe.\n",
    "\n",
    "Assign unique ID and partition ID for each record using Spark SQL. (This is used to distribute data).\n",
    "\n",
    "Sample your data and pull it to the driver node and perform modelling tasks. Create prediction for each instance and display results.\n",
    "\n",
    "Then we use pandas UDFs to scale model. We distribute dataset across spark cluster, and use pyarrow to transfer between spark and pandas dataframe representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How UDFs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first partition a spark dataframe using a groupby statement and each partition is sent to worker node and translated into pandas dataframe and gets passed to UDF.\n",
    "\n",
    "UDF returns a transformed pandas dataframe which is combined with all of the other partitions and then translated back to spark dataframe.\n",
    "\n",
    "pandas UDFs are introduced in spark 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
