{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Class Classification\n",
    "\n",
    "Binary classifiers -- Distinguish between two classes\n",
    "\n",
    "Multiclass classifiers / multinomial classifiers -- distinguish between more than two classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How algorithm trains Multiclass Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest or Naive Bayes are capable of handline multiple class directly.\n",
    "\n",
    "SVM or Linear classifiers are strictly binary classifiers, however there are various strategies that you can use  to perform multiclass classification using binary classifiers.\n",
    "\n",
    "The two Strategies are One-versus-All / One-versus-Rest and One-versus-one.\n",
    "\n",
    "**One-versus-All**: For example, one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (0-detector, 1-detector, 2-detector and so on).\n",
    "\n",
    "Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs highest score.\n",
    "\n",
    "One-vs-all classification is a method which involves training N distinct binary classifiers, each designed for recognizing a particular class. Then those N classifiers are collectively used for multi-class classification.\n",
    "\n",
    "**One-versus-one**: considers each binary pair of classes and trains classifier on subset of data containing those classes. So it trains total n*(n-1)/2 classes. During the classification phases each classifier predicts one class.\n",
    "\n",
    "For example in Mnist data it trains classifiers to distinguish 0s and 1s, another for 0s and 2s, another for 0s and 3s, another for 1s and 2s etc., for mnist problem this means 45 classifiers.\n",
    "\n",
    "The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
    "\n",
    "some algorithms like SVM  scale poorly with the size of the training set, so for these algortithms OvO is preferred since it is faster to train many classifiers on small training sets rather than few classifiers on large training sets. For most binary classification algorithms, OvA is preffered.\n",
    "\n",
    "Scikit-Learn detects when you try to use a binary classification algorithm for multiclass classification task, and it automatically runs OvA (except for SVM classifiers it uses OvO).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass classification for MNIST data\n",
    "\n",
    "#### using One-versus-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('mtrain.csv')\n",
    "train = np.array(train.values)\n",
    "\n",
    "X = train[:, 1:]\n",
    "y = train[:,:1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:35000], X[35000:], y[:35000], y[35000:]\n",
    "\n",
    "shuffle_index = np.random.permutation(35000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ravel = y_train.ravel()\n",
    "y_train_ravel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "some_digit = X[11000] # index 11000, label is 2\n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "sgd_clf.fit(X_train, y_train) \n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code trains the SGDclassifier on the training set using the original target classes from 0 to 9, instead of 2-versus-all target classes (y_train_2). Then is makes a prediction..\n",
    "\n",
    "Underhood Scikit-Learn actually trained 10 binary classifiers, got their decisions scores for the image, and selected the classes with highest score.\n",
    "\n",
    "To see that this is indeed the case, you can call the decision_function() method. Instead of returning just one score per instance, it now returns 10 scores, one per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest score is score related to class 2\n",
    "\n",
    "np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a classifier is trained, it stores the list of target classes in its classes_ attribute, ordered by value. In this case, the index of\n",
    "each class in the classes_ array conveniently matches the class itself (e.g., the class at index 5 happens to be class 5), but in\n",
    "general you won’t be so lucky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision scores for each class. in first case it is 2.\n",
    "# in second case highest value is 3 so the target variable for 3 is digit 3\n",
    "\n",
    "pd.DataFrame(sgd_clf.decision_function([X[11000], X[1233]]), columns=sgd_clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using One-versus-One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use the *OneVsOneClassifier or OneVsRestClassifier* classes. \n",
    "\n",
    "Simply create an instance and pass a binary classifier to its constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier()\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time Scikit-Learn do not have to run OvA or OvO because Random Forest classifiers can directly classify instances into multiple classes.\n",
    "\n",
    "**predict_proba()** to get the list of probabilities that classifier assigned to each instance for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier is fairly confident about its prediction. 1 at 3rd index. \n",
    "\n",
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8 at 7th index means, model estimates 80% probability that the image represents 6 \n",
    "\n",
    "print(forest_clf.predict_proba([X[1200],X[11250], X[100]]))\n",
    "print(forest_clf.predict([X[1200],X[11250], X[100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# accuracy scores in all three folds\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaling to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS you can see scaling improved accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Metrics for Multiclass problems\n",
    "\n",
    "When you have multiclass problem, the average paramter in f1_score, precision, recall needs to be one of the below:\n",
    "\n",
    " 1.weighted\n",
    " \n",
    " 2.micro\n",
    " \n",
    " 3.macro\n",
    " \n",
    "'weighted' calculates de F1 score for each class independently but when it adds them together uses a weight that depends on the number of true labels of each class:\n",
    "\n",
    "   F1class1 ∗ W1 + F1class2 ∗ W2 +⋅⋅⋅+ F1classN∗WN\n",
    "\n",
    "therefore favouring the majority class.\n",
    "\n",
    "\n",
    "'micro' uses the global number of TP, FN, FP and calculates the F1 directly:\n",
    "\n",
    "\n",
    "F1 (class1 + class2 + class3)\n",
    "\n",
    "no favouring any class in particular.\n",
    "\n",
    "Finally, 'macro' calculates the F1 separated by class but not using weights for the aggregation:\n",
    "\n",
    "  F1class1 + F1class2 +⋅⋅⋅+ F1classN\n",
    "\n",
    "which resuls in a bigger penalisation when your model does not perform well with the minority classes.\n",
    "\n",
    "The one to use depends on what you want to achieve. If you are worried with class imbalance I would suggest using 'macro'. However, it might be also worthwile implementing some of the techniques available to taclke imbalance problems such as downsampling the majority class, upsampling the minority, SMOTE, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics for unscaled data with SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\n",
    "\n",
    "def classification_metrics(y_train, y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "    print('f1_score ',f1_score(y_train, y_pred, pos_label='positve', average='weighted'))\n",
    "    print('precision ',precision_score(y_train, y_pred, pos_label='positve', average='weighted'))\n",
    "    print('Recall ',recall_score(y_train, y_pred , pos_label='positve', average='weighted'))\n",
    "\n",
    "classification_metrics(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for scaled data with SGDClassfier\n",
    "\n",
    "You can see how metrics imporved with the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "\n",
    "classification_metrics(y_train, y_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with Random Forest classifier and scaled data\n",
    "\n",
    "Improved accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF = cross_val_predict(forest_clf, X_train_scaled, y_train, cv=3)\n",
    "\n",
    "classification_metrics(y_train, y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# look at the parameters used by our current forest\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(forest_clf.get_params())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10 )]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimim number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1,2,4]\n",
    "\n",
    "# method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each iteration, the algorithm will chose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings.\n",
    "\n",
    "However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use the random grid to search for best hyperparameters\n",
    "# first create the base model to tune\n",
    "# Random search for parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available scores\n",
    "\n",
    "#forest_clf_random = RandomizedSearchCV(estimator=forest_clf, param_distributions=random_grid,\n",
    "                                    # n_iter= 2, cv = 3, verbose=2, random_state=42, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# fit the random search model\n",
    "#forest_clf_random.fit(X_train, y_train_ravel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The most important arguments in the RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv is which is the number of folds to use for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forest_clf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance of base model with random search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "base_accuracy = evaluate(forest_clf, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
