{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are tree shaped diagrams, each node represents a feature, each branch represents a decision and each leaf represents an outcome (categorical or continous). \n",
    "\n",
    "It can be used for both classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Decision Trees Work?\n",
    "\n",
    "**Step 1** Selects the best attribute to split the records. \n",
    "\n",
    "**step 2** Make that attribute a decision node and breaks the data set into smaller subsets. \n",
    "\n",
    "**Step 3** Starts tree building by repeating this process until one of the condition will match. For classification it will construct a set of If-then conditions to split and for Regression split is made based on Sum of Squared Errors.\n",
    "\n",
    "1.All leafs belong to the same class\n",
    "\n",
    "2.There are no remaining attributes\n",
    "\n",
    "3.No more instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is best feature?\n",
    "\n",
    "The best attribute is one which results in the smallest tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How DT selects feature for splitting?\n",
    "\n",
    "To decide which attribute/feature should be placed at root node or internal node or leaf nodes, we use Splitting measures. Most popular splitting measures are \n",
    "\n",
    "1. Information Gain, \n",
    "\n",
    "2. Gain Ratio,\n",
    "\n",
    "3. Gini Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "Entropy is measure of randomness or lack of order or unpredictability in data. It is measure of impurity, higher the entropy, higher the impure data is. When the entropy is equal to 1 it is considered as Maximum split.\n",
    "\n",
    "**Pure data** value of entropy close to zero. That means data data belongs to only one class. A data segment is said to be pure if it contains data instances belonging to just one class. The goal while building decision tree is to reach to a state where leaves (leaf nodes) attain pure state.\n",
    "\n",
    "Information Gain is difference between the entropy of the feature before the split and after the split.\n",
    "\n",
    "InfoGain = E(S1) - E(S2)\n",
    "\n",
    "Higher the difference imples higher the information gain and lower entropy after the split. \n",
    "\n",
    "Decision Tree selects attribute that returns highest Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Ratio\n",
    "\n",
    "Gain ratio is modification of Information Gain. It reduces bias by taking into account the number and size of the branches when choosing attribute.\n",
    "\n",
    "Information gain prefers attributes with large number of distinct values (Ex: Customer_id) this type of attributes provides less information about the class. \n",
    "\n",
    "Even though it provides less information, Information Gain chooses these types of attributes, because these are the attributes with high entropy, when these attributes are split, Entropy is decreased and thus Information Gain will be greater. \n",
    "\n",
    "But the disadvantage here is, it creates useless and lot of number of branches without extracting the useful information. Our goal is to limit size of tree while getting most of information from the attributes.\n",
    "\n",
    "In Gain Ration attribute with maximum Gain ration is selected for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity/Gini Index /Gini Coefficient\n",
    "\n",
    "Suppose we randomly pick a data point in our dataset, and then randomly classify it to any class in the dataset, What is the probability that we classify the data point incorrectly? The answer to the question is Gini Impurity.\n",
    "\n",
    "A `Gini Impurity/Coefficient` ranges between 0 and 1. \n",
    "\n",
    "0 indicates all the instances belong to same class.\n",
    "\n",
    "1 indicates instances randomly distributed across variaous classes\n",
    "\n",
    "`Gini Index` is Gini coefficient expressed as percentage.\n",
    "\n",
    "`Gini Gain` is difference in impurity before the split and after the split.\n",
    "\n",
    "The best feature is the feature which has maximum Gini Gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**criterion** This parameter allows us to use different attribute selection measures.\n",
    "\n",
    "default is 'gini'. It is faster to compute. 'gini' and 'entropy' they both dont have much differences except the gini impurity tends to isolate most frequent classes in its own branch of the tree, while entropy tends to produce slightly more balenced trees.\n",
    "\n",
    "'gini' --> gini index\n",
    "\n",
    "'entropy' --> Information Gain\n",
    "\n",
    "\n",
    "**splitter** This parameter allows us to choose the split strategy.\n",
    "\n",
    "'best' --> best split\n",
    "\n",
    "'random' --> random split.\n",
    "\n",
    "\n",
    "**max_depth** Maximum depth of tree. Higher values of max_depth causes overfitting and low values causes underfitting. max_depth is regularization parameter in Decision Trees.\n",
    "\n",
    "**min_sample_split** the minimum number of samples a nodes must have before split.\n",
    "\n",
    "**min_sample_leaf** minimum number of samples a leaf node must have\n",
    "\n",
    "**max_leaf_nodes** maximum number of leaf nodes.\n",
    "\n",
    "**max_features** maximum number of features that are evaluated for splitting each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "-- Easy to understand.\n",
    "\n",
    "-- No need data preprocessing - Feature scaling.\n",
    "\n",
    "-- Can handle both linear and non-linear data.\n",
    "\n",
    "-- can handle both categorcal and numerical data\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "-- Overfitting - captures noise.\n",
    "\n",
    "-- High Variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134\n",
    "\n",
    "http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf\n",
    "\n",
    "https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
