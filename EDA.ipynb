{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA used to understand and summerize content of a dataset. EDA gives context to develop appropriate model for the problem. EDA Involves following methods,\n",
    "\n",
    "1. Univariate Analysis\n",
    "\n",
    "2. Bivariate Analysis\n",
    "\n",
    "3. Multivariate Analysis\n",
    "\n",
    "4. Dimensionality Reduction\n",
    "\n",
    "5. Feature Engineering\n",
    "\n",
    "Using above methods we can,\n",
    "\n",
    "- Validate Assumptions\n",
    "\n",
    "- Identify patterns and understand problems\n",
    "\n",
    "*Validate Assumptions:* Models and algorithms relies on specific technical assumptions like,\n",
    "\n",
    "- No collinearity between variables\n",
    "\n",
    "- Variance\n",
    "\n",
    "- Data should be normally distributed etc.,\n",
    "\n",
    "These assumptions should be studied before proceeding to model building. If we skip EDA and move onto model building that produces less accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did you perform EDA? What are your goals & outcomes?\n",
    "\n",
    "***Data Profiling:*** In Data Profiling we learn what data contained and assess whether data is suitable for analysis. We do it by learning about data distributions, data quality issues like missing data, outliers and inconsistent data types.\n",
    "\n",
    "We check assumptions required for modelling like Normality, Skewness, Correlation, Multicollinearity, variance etc., and prepare data for model building (Cleaning and feature engineering).\n",
    "\n",
    "***Answering Business & Research Questions:*** Sometimes questions were predetermined. Stakeholders raise questions and we explore data to answer those questions.\n",
    "\n",
    "***Discovery:*** In Discovery we explore data to discover new insights. The goal is to be open-minded and learn what data could tell us. When working with open-ended exploration there were no specific questions involved. \n",
    "\n",
    "Usually we first focus on data profiling then shift focus towards data discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you interact with stakeholders?\n",
    "\n",
    "Stakeholders were,\n",
    "\n",
    "1. Clients\n",
    "\n",
    "2. Data Owners\n",
    "\n",
    "3. Analysis team members\n",
    "\n",
    "***Clients:*** Clients were sometimes internal and external clients. Internal clients were product manager and executives. We often interact with clients in iterative fashion. Besides reporting the final results, we share preliminary results and ask client for feedback such as \n",
    "\n",
    "- Whether results matched their prior knowledge, and \n",
    "\n",
    "- Checking if analysis aligned with project goals.\n",
    "\n",
    "***Data Owners:*** We usually interact with data engineers and data administrators who perform ETL, Process and store data. We ask data owners to provide additional information to locate, clean and understand data since they have better understanding of the Format, Meaning and location of data.\n",
    "\n",
    "***Team Members:*** We regularly take feedback from team before presenting results to clients. Typical feedback include,\n",
    "\n",
    "- Additonal questions to explore,\n",
    "\n",
    "- Technical advice on techniques used,\n",
    "\n",
    "- Suggestions on making reports easy to understand for clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What tasks did you perform during data analysis? What are the challenges in each task?\n",
    "\n",
    "Data Analysis is iterativ process that couples following processes,\n",
    "\n",
    "1. Data Aquisition\n",
    "\n",
    "2. Data Wrangling or Exploration\n",
    "\n",
    "3. Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges in Data Aquisition:\n",
    "\n",
    "When working with existing data, finding relevant data is difficult because data is often distributed in multiple storage infrastructures. We have to search for data in many places. \n",
    "\n",
    "Moreover data sources had often insufficient descriptions, uninformative column labels, missing and outdated documentation. As a result, we have to explore all potential datasets to access if they were relevant to their analysis.\n",
    "\n",
    "When people are working from offshore, it will be difficult for them to find right people to talk.\n",
    "\n",
    "We used keyword search to look for relavent datasets in databases. However same data is named in many ways it is difficult to find specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges in Data Exploration\n",
    "\n",
    "We have to use many different scripting languages to fetch data from different platforms. Data came from sources had improper format and size for analysis tools. Then we need to transform data prior to exploration, we spend more time only on cleaning. \n",
    "\n",
    "After getting the data, We need to join multiple datasets from multiple sources, it presented many challenges. One of the challenge is inconsistency between data sources, various datasets have various timestamps. We spend few days just to get data on the same time systems before combining them. \n",
    "\n",
    "After dealing with inconsistencies, new task is to combine data from multiple tables, we often need to combine data from 20 tables. \n",
    "\n",
    "Moreover, sometimes exploration tools lack support for sime wrangling tasks, they have to switch between tools throughout the analysis and migrate data between tools. \n",
    "\n",
    "We have to convert data into format expected by the analysis tools. Common formatting tasks include \n",
    "\n",
    "- converting file formats, \n",
    "\n",
    "- character encoding, \n",
    "\n",
    "- manipulating data layout such as splitting columns & reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges in Reporting\n",
    "\n",
    "We have to produce reports that are simple and easier to understand. We avoid using sophisticated plots such as box plots for stakeholders with less data analysis experience. While exploration might have many delicate things they are often presented with only most important findings, which are more critical for decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to choose variables to explore?\n",
    "\n",
    "Exploring data with 100s of features is challenging. We need to plot many variable combinations. Choosing variables is harder than plotting, it is too time consuming.\n",
    "\n",
    "Data Analysts need domain knowlege to know where the data is coming from and how it is processed and to interpret data and detech errors. We can take suggestions from different stakeholders to get domain knowledge.\n",
    "\n",
    "Discard and drop the databases and datasets that are irrelavant to study. Drop features that have data quality issues. Use statistical modelling techniques to select variables such as Random Forest, Decision Trees to determine important features.\n",
    "\n",
    "Somtimes we might need to use Dimensionality reduction techniques like PCA and t-SNE and combining 1000s of different featues leades to interpretation difficulties. We cant explain audience what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did you dealt with repetitive tasks?\n",
    "\n",
    "Data Analysts need to do repetitive tasks like Imputation, Visualizations. We need to do nothing new except changing colours that are more attractive and easy to understand. \n",
    "\n",
    "To deal with repetitive tasks we compile templates of code that is often used. We use that code to generate basic summary of variables, run basic data quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When does EDA End?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the common task is deciding when to end EDA. We decide to end EDA based on many factors such as goal satisfaction, feedback from stakeholders, time constraints.\n",
    "\n",
    "For profiling we stop EDA after verifying all the assumptions and we end EDA when we have intuition how to formulate models on top of data.\n",
    "\n",
    "We often use feedback from teammates and stakeholders to assess need to further explore data. We would stop once they have no concerns about data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving average filter:\n",
    "\n",
    "It is a low pass filter smpoothing an array of sampled data/signal. Used mainly in IoT data and financial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation:\n",
    "\n",
    "Log transformation is widely used to address skewed data. Most of the data upon which statistical tests are performed is often skewed. When we use skewed data without considering assumption that these tests need normally distributed data, they yield incorrect results.\n",
    "\n",
    "In order to avoid that we have to transform data into normal distribution. Log transformation is not most popular among different types of transformations used to transform skewed data into approximately \"Normal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
